{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import CategoricalEncoder\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, GRU, Activation, Dropout\n",
    "from keras import layers\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "from hyperopt import fmin, hp, Trials, tpe, STATUS_OK\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the functions to pre-process the data before feeding it\n",
    "\n",
    "def split_data(data, training_size=0.8):\n",
    "  \"\"\"\n",
    "  data: Pandas Dataframe\n",
    "  training_size: proportion of the data to be used for training\n",
    "  This function splits the data into training_set and test_set based on the given training_size\n",
    "  Return: train_set and test_set as pandas DataFrame\n",
    "  \"\"\"\n",
    "  return data[:int(training_size*len(data))], data[int(training_size*len(data)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importer le csv, quelques erreurs pouvaient exister \n",
    "\n",
    "dataset_path = r'C:\\Users\\maxim\\Desktop\\Work\\Hackaton\\Dataset\\20181204_mines-paristech_hackathon-input\\hack_train.csv'\n",
    "dataset_raw = pd.read_csv(dataset_path, header = 0,sep = ';', error_bad_lines = False,encoding = \"ISO-8859-1\")\n",
    "\n",
    "# shuffles the dataset\n",
    "dataset_raw = dataset_raw.sample(frac=1).reset_index(drop=True)\n",
    "training_set, test_set = split_data(dataset_raw)\n",
    "\n",
    "# fits the count vectorizer on training data\n",
    "count_vec = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "descriptions_training = training_set['description']\n",
    "descriptions_training.fillna(value='', inplace=True)\n",
    "\n",
    "count_vec.fit(descriptions_training)\n",
    "\n",
    "transformer.fit(count_vec.transform(descriptions_training))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using bag of words technique\n",
    "\n",
    "def process_bag(descriptions):\n",
    "\n",
    "    x = count_vec.transform(descriptions)\n",
    "    x = transformer.transform(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_inputs_outputs(data):\n",
    "    \"\"\"\n",
    "    data: pandas DataFrame, this could be either training_set or test_set\n",
    "    This function will create input array X from the given dataset and will normalize 'Close' and 'Volume' between 0 and 1\n",
    "    Return: \n",
    "    data.values -> the input except description\n",
    "    description -> the preprocessed description\n",
    "    and output  -> the 'sold' booleans\n",
    "    \"\"\"\n",
    "\n",
    "    output = data['sold']\n",
    "\n",
    "    descriptions = data['description']\n",
    "    descriptions.fillna(value='', inplace=True)\n",
    "    descriptions = process_bag(descriptions)\n",
    "    \n",
    "    # we have to drop not relevant input and put the description apart\n",
    "    data = data.drop(['description','UniqueID','sold'],1)\n",
    "\n",
    "    # here we transform the categorical inputs in one-hot vectors\n",
    "    for name in (data.columns):\n",
    "      if name not in ['startprice', 'description']:\n",
    "        one_hot = pd.get_dummies(data[name], prefix = name)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        data = data.drop(name,1)\n",
    "\n",
    "    return data.values, descriptions, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "918\n"
     ]
    }
   ],
   "source": [
    "# divides the dataset into training and test\n",
    "training_set, test_set = split_data(dataset_raw)\n",
    "\n",
    "#descriptions = training_set['description']\n",
    "#descriptions.fillna(value='', inplace=True)\n",
    "#descriptions = process_bag(descriptions)\n",
    "\n",
    "#print(descriptions[2])\n",
    "\n",
    "X_train, D_train, Y_train =  create_inputs_outputs(training_set)\n",
    "X_test,D_test, Y_test = create_inputs_outputs(test_set)\n",
    "\n",
    "features_shape = X_train.shape[1]\n",
    "descriptions_shape = D_train.shape[1]\n",
    "\n",
    "print(features_shape)\n",
    "print(descriptions_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modele(l=100, m=50, n=30, o=20, dropout_rate = 0.2):\n",
    "        input_features = Input((features_shape,))\n",
    "        input_descriptions = Input((descriptions_shape,))\n",
    "    \n",
    "        desc = input_descriptions\n",
    "        feat = input_features\n",
    "        \n",
    "        desc = Dense(int(l), activation='relu')(desc)\n",
    "        desc = Dropout(dropout_rate)(desc)\n",
    "        desc = Dense(int(m), activation='relu')(desc)\n",
    "        \n",
    "        x = Concatenate(axis = -1)([feat, desc])\n",
    "        \n",
    "        x = Dense(int(n), activation='relu')(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Dense(int(0), activation='relu')(x)\n",
    "        x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "        model = Model(inputs = [input_features, input_descriptions], outputs = X, name='model')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-99853733408f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#hyperparam_opt()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodele1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodele\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodele1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"binary_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodele1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-7ead0134b59a>\u001b[0m in \u001b[0;36mmodele\u001b[1;34m(l, m, n, o, dropout_rate)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    472\u001b[0m             if all([s is not None\n\u001b[0;32m    473\u001b[0m                     for s in to_list(input_shape)]):\n\u001b[1;32m--> 474\u001b[1;33m                 \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maxim\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m         \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m         \u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#hyperparam_opt()\n",
    "\n",
    "modele1 = modele(l=100,m=50,n=30,o=20, dropout_rate=0.2)\n",
    "modele1.compile(optimizer = SGD(0.00001),loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "history = modele1.fit(inputs = [X_train, D_train], y = Y_train, epochs = 100, batch_size = 1000, validation_data = ([X_test,D_test], Y_test))\n",
    "modele1.evaluate(x = features_test, y = sold_test)\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
